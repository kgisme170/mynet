Spark dataframe requires json file as one object in one line?

I'm new to spark and trying to use spark to read json file like this. Using spark 2.3 and scala 2.11 on ubuntu18.04, java1.8:

cat my.json:

    { "Name":"A", "No_Of_Emp":1, "No_Of_Supervisors":2}
    { "Name":"B", "No_Of_Emp":2, "No_Of_Supervisors":3}
    { "Name":"C", "No_Of_Emp":13,"No_Of_Supervisors":6}

And my scala code is:

    val dir = System.getProperty("user.dir")
    val conf = new SparkConf().setAppName("spark sql")
    .set("spark.sql.warehouse.dir", dir)
    .setMaster("local[4]");
    val spark = SparkSession.builder().config(conf).getOrCreate()
    val df = spark.read.json("my.json")
    df.show()
    df.printSchema()
    df.select("Name").show()

OK, everything is fine. But if I change the json file to be multiline, standard json format:

    [
        {
          "Name": "A",
          "No_Of_Emp": 1,
          "No_Of_Supervisors": 2
        },
        {
          "Name": "B",
          "No_Of_Emp": 2,
          "No_Of_Supervisors": 3
        },
        {
          "Name": "C",
          "No_Of_Emp": 13,
          "No_Of_Supervisors": 6
        }
    ]

Then the program will report error:

    +--------------------+
    |     _corrupt_record|
    +--------------------+
    |                   [|
    |                   {|
    |        "Name": "A",|
    |      "No_Of_Emp"...|
    |      "No_Of_Supe...|
    |                  },|
    |                   {|
    |        "Name": "B",|
    |      "No_Of_Emp"...|
    |      "No_Of_Supe...|
    |                  },|
    |                   {|
    |        "Name": "C",|
    |      "No_Of_Emp"...|
    |      "No_Of_Supe...|
    |                   }|
    |                   ]|
    +--------------------+

    root
     |-- _corrupt_record: string (nullable = true)

    Exception in thread "main" org.apache.spark.sql.AnalysisException: cannot resolve '`Name`' given input columns: [_corrupt_record];;
    'Project ['Name]
    +- Relation[_corrupt_record#0] json

I wish to know why this happens? A none standard json file without double [] will work(one object one line), but a more standardized formatted json will be a "corrupt record"?

----------
Spark dataFrame doesn't have createGlobalTempView() function?
I'm using spark2.3 and scala 2.11. I created dataFrame and can use createOrReplaceTempView() function. But seems createGlobalTempView() function is missing, intellij says it can't find such method, nor could the intellisense give me the list. So I wish to know if this method is obsoleted?

Thanks!

----------

Spark sql programmatic schema throws exception:

I'm trying to use "Programmatically Specifying the Schema"(http://spark.apache.org/docs/latest/sql-programming-guide.html).
I have this my.txt file as input:

    "A", 1, 2
    "B", 2, 3
    "B", 13,6
    "C", 3, 4

Then I have a spark program to read and interpret it:

    val rowRdd = spark.sparkContext.textFile("my.txt")
    .map(_.split(","))
    .map(a => Row(a(0), a(1).trim.toInt, a(2).trim.toInt))
    val schema = StructType("a b c".split(" ")
    .map(f => StructField(f, StringType, nullable = true)))
    spark.createDataFrame(rowRdd, schema).createOrReplaceTempView("people")
    spark.sql("select * from people").show()

When running with spark-submit, it gives exception:

    java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of string
    if (assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, a), StringType), true) AS a#0
    +- if (assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object), 0, a), StringType), true)
       :- assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object).isNullAt
       :  :- assertnotnull(input[0, org.apache.spark.sql.Row, true], top level row object)
       :  :  +- input[0, org.apache.spark.sql.Row, true]
       :  +- 0
    ... ...(many lines)

Where did I get wrong in my program and how to fix it? Thanks a lot.